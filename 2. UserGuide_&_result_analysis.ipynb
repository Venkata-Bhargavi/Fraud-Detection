{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Overview\n",
    "\n",
    "This series of experiments aims to:\n",
    "\n",
    "1. **Understand the Data:** Analyze the dataset to identify key features and patterns related to fraud.\n",
    "2. **Find Insights:** Discover data patterns and anomalies that indicate fraudulent transactions.\n",
    "3. **Model Effectiveness:** Evaluate various machine learning models (Logistic Regression, Random Forest, CatBoost, Decision Tree) to assess the impact of feature selection and class balancing on fraud detection performance.\n",
    "\n",
    "The goal is to combine data understanding with effective modeling to improve fraud detection capabilities.\n",
    "\n",
    "The notebook `Load_&_Preprocess.ipynb` handles the following:\n",
    "- **Data Loading:** Reads and imports the data.\n",
    "- **Data Transformation:** Applies appropriate encoding and feature selection to prepare the data for model training.\n",
    "- **Flagging Transactions:** Implements logic to flag reversal and multi-swipe transactions, which are crucial for identifying patterns associated with fraud.\n",
    "\n",
    "------\n",
    "\n",
    "### Experimental Setup\n",
    "\n",
    "Each model was evaluated under four different conditions, with corresponding Jupyter notebooks documenting the experiments:\n",
    "\n",
    "1. **Feature Selection and Class Balancing Enabled** (`Feature Selection = TRUE`, `Class Balancing = TRUE`):\n",
    "   - **File**: `Modelling_Selected_Features_Balanced.ipynb`\n",
    "   - **Description**: This notebook contains the results for all four models using a set of features selected through feature engineering. The class imbalance was addressed using a `RandomUnderSampler`. This setup represents an optimized approach where irrelevant features are removed, and the dataset is balanced to improve model performance.\n",
    "\n",
    "2. **Feature Selection Enabled, Class Balancing Disabled** (`Feature Selection = TRUE`, `Class Balancing = FALSE`):\n",
    "   - **File**: `Modelling_Selected_Features_Unbalanced.ipynb`\n",
    "   - **Description**: This notebook includes results for all four models with selected features but without any class balancing. The goal here was to observe the effect of feature selection on model performance when the class imbalance is not addressed.\n",
    "\n",
    "3. **Feature Selection Disabled, Class Balancing Enabled** (`Feature Selection = FALSE`, `Class Balancing = TRUE`):\n",
    "   - **File**: `Modelling_All_Features_Balanced.ipynb`\n",
    "   - **Description**: This notebook presents results for all four models using all available features (i.e., no feature selection was performed). Class balancing was applied using a `RandomUnderSampler` to see how the models perform when no feature reduction is done but the data is balanced.\n",
    "\n",
    "4. **Feature Selection and Class Balancing Disabled** (`Feature Selection = FALSE`, `Class Balancing = FALSE`):\n",
    "   - **File**: `Modelling_All_Features_Unbalanced.ipynb`\n",
    "   - **Description**: This notebook contains results for all four models using the full set of features without any class balancing. It serves as a baseline to understand how the models perform with all features and without addressing class imbalance.\n",
    "\n",
    "\n",
    "*Note: All the results from each of above experiments are stored at `data/result_df.csv'*\n",
    "\n",
    "------\n",
    "\n",
    "## Execution Instructions\n",
    "\n",
    " To install all libraries use `pip install -r requirements.txt`\n",
    "\n",
    "1. **Run the Data Preprocessing Notebook:**\n",
    "   - Execute the `Load_&_Preprocess.ipynb` notebook.\n",
    "   - After running this notebook, two CSV files will be saved in the `data` folder:\n",
    "     - `transformed_data_all_features.csv`\n",
    "     - `transformed_data_selected_features.csv`\n",
    "   - These CSV files can be used in the modeling process wherever required.\n",
    "\n",
    "2. **Run the Modeling Notebooks:**\n",
    "   - Start with `3.1 Modelling_Selected_Features_Balanced.ipynb`, which creates the `result_df` DataFrame to store all results and updates it accordingly.\n",
    "   - Next, run the following notebooks in sequence:\n",
    "     - `3.2 Modelling_Selected_Features_Unbalanced.ipynb`\n",
    "     - `3.3 Modelling_All_Features_Balanced.ipynb`\n",
    "     - `3.4 Modelling_All_Features_Unbalanced.ipynb`\n",
    "   - Running all these notebooks will generate results for all combinations of feature selection and class balancing.\n",
    "\n",
    "3. **Results Storage:**\n",
    "   - At the end of the process, all the updated results are stored in the `data/result_df.csv` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Feature Selection</th>\n",
       "      <th>Class Balancing</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Train_Precision</th>\n",
       "      <th>Train_Recall</th>\n",
       "      <th>Test_Precision</th>\n",
       "      <th>Test_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Feature Selection  Class Balancing   AUC  \\\n",
       "0   Logistic Regression               True             True  0.70   \n",
       "1         Random Forest               True             True  0.73   \n",
       "2              CatBoost               True             True  0.74   \n",
       "3         Decision Tree               True             True  0.70   \n",
       "4   Logistic Regression               True            False  0.70   \n",
       "5         Random Forest               True            False  0.73   \n",
       "6              CatBoost               True            False  0.74   \n",
       "7         Decision Tree               True            False  0.72   \n",
       "8   Logistic Regression              False             True  0.69   \n",
       "9         Random Forest              False             True  0.74   \n",
       "10             CatBoost              False             True  0.81   \n",
       "11        Decision Tree              False             True  0.71   \n",
       "12  Logistic Regression              False            False  0.67   \n",
       "13        Random Forest              False            False  0.75   \n",
       "14             CatBoost              False            False  0.81   \n",
       "15        Decision Tree              False            False  0.75   \n",
       "\n",
       "    Train_Precision  Train_Recall  Test_Precision  Test_Recall  \n",
       "0              0.65          0.66            0.64         0.66  \n",
       "1              0.70          0.64            0.69         0.61  \n",
       "2              0.71          0.70            0.67         0.67  \n",
       "3              0.72          0.73            0.64         0.66  \n",
       "4              0.00          0.00            0.00         0.00  \n",
       "5              0.00          0.00            0.00         0.00  \n",
       "6              0.80          0.00            0.00         0.00  \n",
       "7              0.96          0.02            0.06         0.00  \n",
       "8              0.65          0.65            0.63         0.64  \n",
       "9              0.68          0.70            0.66         0.69  \n",
       "10             0.82          0.88            0.72         0.73  \n",
       "11             0.76          0.78            0.66         0.69  \n",
       "12             0.00          0.00            0.00         0.00  \n",
       "13             0.00          0.00            0.00         0.00  \n",
       "14             0.94          0.20            0.61         0.03  \n",
       "15             0.86          0.04            0.25         0.01  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.read_csv(\"data/result_df.csv\")\n",
    "result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 3 Performing Models:\n",
    "\n",
    "### 1st Place: CatBoost without Feature Selection but with Class Balancing (Row 10)\n",
    "- **AUC:** ~0.81\n",
    "- **Precision:** ~0.72\n",
    "- **Recall:** ~0.73\n",
    "- **Summary**: This model achieves the highest AUC, precision, and recall among all experiments, making it the best overall performer. It effectively balances precision and recall, which is crucial for minimizing false positives and false negatives in fraud detection.\n",
    "\n",
    "### 2nd Place: CatBoost with Feature Selection and Class Balancing (Row 2)\n",
    "- **AUC:** ~0.74\n",
    "- **Precision:** ~0.67\n",
    "- **Recall:** ~0.67\n",
    "- **Summary**: This model ranks second due to its strong balance between AUC, precision, and recall. The feature selection combined with class balancing contributes to its effectiveness in distinguishing fraudulent transactions.\n",
    "\n",
    "### 3rd Place: Random Forest without Feature Selection but with Class Balancing (Row 9)\n",
    "- **AUC:** ~0.74\n",
    "- **Precision:** ~0.66\n",
    "- **Recall:** ~0.69\n",
    "- **Summary**: This model also performs well, closely following the CatBoost models. It demonstrates that even without feature selection, class balancing can significantly enhance the model's ability to detect fraud.\n",
    "\n",
    "> The top-performing models show consistent results across both training and test sets, indicating robustness and generalization. This consistency suggests that these models are not only performing well on the training data but are also likely to perform reliably on unseen data, which is crucial for real-world applications like fraud detection.\n",
    "\n",
    "## Important Metrics for Predicting Fraud:\n",
    "- **AUC (Area Under the Curve):**\n",
    "  - **Importance**: Measures the model's ability to differentiate between fraudulent and non-fraudulent transactions. A higher AUC is essential for effective fraud detection.\n",
    "  - **Top Models:**\n",
    "    - **1st:** CatBoost without feature selection but with class balancing (~0.80)\n",
    "    - **2nd:** CatBoost with feature selection and class balancing (~0.74)\n",
    "    - **3rd:** Random Forest without feature selection but with class balancing (~0.74)\n",
    "\n",
    "- **Precision:**\n",
    "  - **Importance**: Indicates the accuracy of the model in predicting fraud cases, minimizing false positives.\n",
    "  - **Top Models:**\n",
    "    - **1st:** CatBoost without feature selection but with class balancing (~0.72)\n",
    "    - **2nd:** CatBoost with feature selection and class balancing (~0.68)\n",
    "    - **3rd:** Random Forest without feature selection but with class balancing (~0.67)\n",
    "\n",
    "- **Recall:**\n",
    "  - **Importance**: Measures how well the model identifies actual fraud cases, which is critical for reducing false negatives.\n",
    "  - **Top Models:**\n",
    "    - **1st:** CatBoost without feature selection but with class balancing (~0.73)\n",
    "    - **2nd:** CatBoost with feature selection and class balancing (~0.67)\n",
    "    - **3rd:** Random Forest without feature selection but with class balancing (~0.66)\n",
    "\n",
    "> **Note:** \n",
    "> In fraud detection, recall is particularly important. It is similar to cancer prediction, where failing to detect cancer (or fraud in this case) can have severe consequences. If a non-fraudulent transaction is flagged as fraud, it only incurs the cost of additional verification or inquiry. However, if a fraudulent transaction is not detected, it can result in significant financial losses, potentially amounting to millions of dollars."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71dc8da66586bbf0d1ccb46dde7852623b5aaaca097c0d2f6ed9fe6bbd813df9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
